{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: TensorFlow and Edge Detection using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# from __future__ import division\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from utils import *\n",
    "from HOG import *\n",
    "from svm_estimator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eval_neg', 'train_neg', 'test_pos', 'eval_pos', 'train_pos', 'test_neg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data and start an InteractiveSession\n",
    "sess = tf.InteractiveSession()\n",
    "datasets = np.load('pedestrian_dataset.npz')    # extracted from the original Dalal and Triggs paper dataset available here: http://pascal.inrialpes.fr/data/human/\n",
    "[k for k in datasets.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train_neg'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128, 64, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([datasets['train_neg'], datasets['train_pos']], axis = 0).shape    # should be of size [#datapoints, 1152] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d716d5ceeb34df2b70ae51777b6f455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0yNDksIGRlc2NyaXB0aW9uPXUnaycsIG1heD00OTkpLCBJbnRTbGlkZXIodmFsdWU9MCwgZGVzY3JpcHRpb249dSdwJywgbWHigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### EXPLORE THE DATASET!\n",
    "# p=0 are negative examples\n",
    "# p=1 are positive examples\n",
    "@interact(k = (0, datasets[\"train_neg\"].shape[0] - 1), p = (0, 1))\n",
    "def view_img(k, p):\n",
    "    d = {0: \"train_neg\", 1: \"train_pos\"}\n",
    "    plt.imshow(datasets[d[p]][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73aa6c806a6a4a3597cdbd93647c118b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0yNDksIGRlc2NyaXB0aW9uPXUnaycsIG1heD00OTkpLCBJbnRTbGlkZXIodmFsdWU9MCwgZGVzY3JpcHRpb249dSdwJywgbWHigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### VISUALIZE HOGs!\n",
    "# p=0 are negative examples\n",
    "# p=1 are positive examples\n",
    "@interact(k = (0, datasets[\"train_neg\"].shape[0]-1), p = (0, 1))\n",
    "def view_img(k, p):\n",
    "    d = {0: \"train_neg\", 1: \"train_pos\"}\n",
    "    plt.figure(figsize = (15,15))    # feel free to change this depending on your screen resolution\n",
    "    plt.imshow(datasets[d[p]][k])\n",
    "    # we can call .eval() instead of sess.run() below since we're using an InteractiveSession\n",
    "    plot_cell_hogs(tf_histogram_of_oriented_gradients(datasets[d[p]][k])[0].eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do this after you have trained your SVM with HOG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 1000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1863f7a3d0>, '_model_dir': 'training_checkpoints/hog', '_protocol': None, '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 1000, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "Loading model from: training_checkpoints/hog/model/1549736789\n",
      "INFO:tensorflow:Restoring parameters from training_checkpoints/hog/model/1549736789/variables/variables\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb738960bf3c4619ad98ef12ceb1897d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT05OSwgZGVzY3JpcHRpb249dSdrJywgbWF4PTE5OSksIEludFNsaWRlcih2YWx1ZT0wLCBkZXNjcmlwdGlvbj11J3Nob3dfaW3igKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### VISUALIZE WEIGHTED HOGs!\n",
    "##################################\n",
    "# ADD MODEL NUMBER IN training_checkpoints/hog/model\n",
    "model_number = 1549736789\n",
    "##################################\n",
    "assert model_number is not None, \"ADD MODEL NUMBER IN training_checkpoints/hog/model\"\n",
    "\n",
    "(x_train, y_train), (x_eval, y_eval), (x_pred, y_true) = get_hog_data()\n",
    "data = {}\n",
    "data['x_train'] = x_train\n",
    "data['y_train'] = y_train\n",
    "data['x_eval'] = x_eval\n",
    "data['y_eval'] = y_eval\n",
    "data['x_pred'] = x_pred\n",
    "data['y_true']  = y_true\n",
    "data['name'] = 'hog'\n",
    "estimator_model = svm(data, load=True)\n",
    "model_dir = \"training_checkpoints/hog/model/\" + str(model_number)\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    sess = tf.Session()\n",
    "    print(\"Loading model from: \" + model_dir)\n",
    "    tf.saved_model.loader.load(sess,\n",
    "                               [tf.saved_model.tag_constants.SERVING],\n",
    "                               model_dir)\n",
    "\n",
    "data = np.concatenate([datasets[\"test_pos\"], datasets[\"test_neg\"]], axis=0)\n",
    "\n",
    "weights = np.reshape(estimator_model.get_variable_value('svm/weights'), [1, -1])\n",
    "pos_weights = np.maximum(weights, 0)\n",
    "neg_weights = -np.minimum(weights, 0)\n",
    "file_exists = False\n",
    "try:\n",
    "    misclass_idx = np.load(\"hog_misclass_idx.npy\")\n",
    "    file_exists = True\n",
    "except OSError:\n",
    "    print(\"\\n\\nYou need to run the svm with HOG features first! hog_misclass_idx.py doesn't exist yet.\\n\\n\")\n",
    "\n",
    "if file_exists:\n",
    "    @interact(k = (0, data.shape[0]-1), show_image = (False, True))\n",
    "    def view_img(k, show_image):\n",
    "\n",
    "        misclass_idx = np.load(\"hog_misclass_idx.npy\")\n",
    "        plt.figure(figsize = (15,15))    # feel free to change this depending on your screen resolution\n",
    "        plt.imshow(data[k])\n",
    "        hog = hog_descriptor(data[k]).eval()\n",
    "        unweighted_block_hogs = np.mean(hog.reshape([16, 8, 1, 9]), axis=2)\n",
    "        pos_weighted_block_hogs = np.mean(pos_weights.reshape([16, 8, 1, 9])*hog.reshape([16, 8, 1, 9]), axis=2)\n",
    "        neg_weighted_block_hogs = np.mean(neg_weights.reshape([16, 8, 1, 9])*hog.reshape([16, 8, 1, 9]), axis=2)\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        color = 'yellow' if show_image else 'white'\n",
    "        plt.imshow(show_image*data[k])\n",
    "        plot_cell_hogs(unweighted_block_hogs, pixels_in_cell=8, color=color)\n",
    "        if k in misclass_idx:\n",
    "            plt.title(\"Misclassified!\")\n",
    "        else:\n",
    "            plt.title(\"Classified correctly\")\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(show_image*data[k])\n",
    "        plot_cell_hogs(pos_weighted_block_hogs, pixels_in_cell=8, color=color)\n",
    "        plt.title(\"Positive Weights\")\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(show_image*data[k])\n",
    "        plot_cell_hogs(neg_weighted_block_hogs, pixels_in_cell=8, color=color)\n",
    "        plt.title(\"Negative Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
